{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arockiasachin/ContextualFinAi/blob/main/RAG_using_LlamaIndex_Faiss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U llama_index langchain langchain-community"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DC4kXiPjxm-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers bitsandbytes accelerate sqlparse langchain faiss-cpu pypdf tiktoken databutton llama-index-vector-stores-faiss llama-index-embeddings-huggingface sentence-transformers llama-index-embeddings-langchain llama-index-llms-ollama llama-index-llms-huggingface llama-index-llms-huggingface-api  google-generativeai llama-index-llms-gemini"
      ],
      "metadata": {
        "id": "oz9QpAMGceWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "0XddSm7pw-IA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from typing import List\n",
        "from pypdf import PdfReader\n",
        "from llama_index.core import Document, VectorStoreIndex, StorageContext, load_index_from_storage, Settings\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from llama_index.embeddings.langchain import LangchainEmbedding\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "vbDph6HI1Qnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ingestion**"
      ],
      "metadata": {
        "id": "Z2Pzn-8y1_8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize embedding model and global settings\n",
        "lc_embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "embed_model = LangchainEmbedding(lc_embed_model)\n",
        "\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 512\n",
        "Settings.chunk_overlap = 20\n",
        "\n",
        "# Directory for index persistence\n",
        "PERSIST_DIR = \"./faiss_storage\"\n",
        "\n",
        "# Upload and process PDFs\n",
        "def upload_and_process_pdfs():\n",
        "    uploaded = files.upload()\n",
        "    pdf_file_names = list(uploaded.keys())\n",
        "    pdf_file_contents = []\n",
        "\n",
        "    for file_name in pdf_file_names:\n",
        "        pdf_file_contents.append(BytesIO(uploaded[file_name]))\n",
        "    return pdf_file_names, pdf_file_contents\n",
        "\n",
        "# Parse PDF text by page\n",
        "def parse_pdf(file: BytesIO, filename: str) -> List[str]:\n",
        "    pdf = PdfReader(file)\n",
        "    return [page.extract_text().replace(\"-\\n\", \"\").replace(\"\\n\", \" \").strip() for page in pdf.pages]\n",
        "\n",
        "# Convert text to Document format\n",
        "def text_to_docs(text: List[str], filename: str) -> List[Document]:\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=0)\n",
        "    doc_chunks = []\n",
        "    for i, page_text in enumerate(text):\n",
        "        chunks = text_splitter.split_text(page_text)\n",
        "        doc_chunks.extend(\n",
        "            Document(text=chunk, metadata={\"page\": i + 1, \"chunk\": j, \"filename\": filename})\n",
        "            for j, chunk in enumerate(chunks)\n",
        "        )\n",
        "    return doc_chunks\n"
      ],
      "metadata": {
        "id": "4fjlg05I1Nkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Build and Load VectorStore**"
      ],
      "metadata": {
        "id": "f9ugOTeY2JYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and save FAISS index with persistence\n",
        "def build_and_save_faiss_index(documents: List[Document]):\n",
        "    # Clear and create persistence directory\n",
        "    if os.path.exists(PERSIST_DIR):\n",
        "        shutil.rmtree(PERSIST_DIR)\n",
        "    os.makedirs(PERSIST_DIR, exist_ok=True)\n",
        "\n",
        "    # Initialize FAISS index\n",
        "    embedding_dim = len(embed_model.get_text_embedding(documents[0].text))\n",
        "    faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "\n",
        "    # Set up vector store and document storage context\n",
        "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "    # Add documents to the index\n",
        "    index = VectorStoreIndex.from_documents(documents, storage_context=storage_context)\n",
        "\n",
        "    # Persist the index to disk\n",
        "    storage_context.persist(persist_dir=PERSIST_DIR)\n",
        "    return index\n",
        "\n",
        "# Load FAISS vector store index from disk\n",
        "def load_faiss_index():\n",
        "    vector_store = FaissVectorStore.from_persist_dir(PERSIST_DIR)\n",
        "    storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=PERSIST_DIR)\n",
        "    return load_index_from_storage(storage_context=storage_context)\n",
        "\n",
        "# Main function to upload, parse, and build an index for PDFs\n",
        "def get_index_for_pdfs():\n",
        "    pdf_file_names, pdf_file_contents = upload_and_process_pdfs()\n",
        "    documents = []\n",
        "    for pdf_file, pdf_name in zip(pdf_file_contents, pdf_file_names):\n",
        "        text = parse_pdf(pdf_file, pdf_name)\n",
        "        documents.extend(text_to_docs(text, pdf_name))\n",
        "    return build_and_save_faiss_index(documents)\n"
      ],
      "metadata": {
        "id": "NRJLvdbEsJL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fucntion Call**"
      ],
      "metadata": {
        "id": "IOPhlP0y2P9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage: Upload, build, and load the index\n",
        "index = get_index_for_pdfs()\n",
        "loaded_index = load_faiss_index()"
      ],
      "metadata": {
        "id": "__IEV8kg1HHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Querstion Answering/RAG**"
      ],
      "metadata": {
        "id": "TzLcpXyzTkHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GOOGLE_API_KEY = \"insert Gemini AI key here\" #get it from https://aistudio.google.com/app/apikey\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ],
      "metadata": {
        "id": "8f241Gcg-KS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.llms.gemini import Gemini\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Configure global settings to use Gemini\n",
        "Settings.llm = Gemini(model=\"models/gemini-pro\")"
      ],
      "metadata": {
        "id": "0A5P7_Oj-K1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query example\n",
        "query_engine = loaded_index.as_query_engine()\n",
        "response = query_engine.query(\"What are main topics being disscussed in this document\")\n",
        "print(response)"
      ],
      "metadata": {
        "id": "V-D192MqqDJm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}